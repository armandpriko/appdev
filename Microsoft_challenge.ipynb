{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8928842,
          "sourceType": "datasetVersion",
          "datasetId": 5371065
        },
        {
          "sourceId": 9020191,
          "sourceType": "datasetVersion",
          "datasetId": 5435641
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebookc29b058c63",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandpriko/appdev/blob/main/Microsoft_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-25T04:19:11.776804Z",
          "iopub.execute_input": "2024-07-25T04:19:11.777635Z",
          "iopub.status.idle": "2024-07-25T04:19:20.839212Z",
          "shell.execute_reply.started": "2024-07-25T04:19:11.777601Z",
          "shell.execute_reply": "2024-07-25T04:19:20.838219Z"
        },
        "trusted": true,
        "id": "-wH1Htm-UArF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"/kaggle/input/zindinlp/Test.csv\")\n",
        "train = pd.read_csv(\"/kaggle/input/zindinlp/Train.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:19:30.34268Z",
          "iopub.execute_input": "2024-07-25T04:19:30.343903Z",
          "iopub.status.idle": "2024-07-25T04:19:30.589066Z",
          "shell.execute_reply.started": "2024-07-25T04:19:30.343867Z",
          "shell.execute_reply": "2024-07-25T04:19:30.588155Z"
        },
        "trusted": true,
        "id": "0Q93MkKcUArK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:19:30.594586Z",
          "iopub.execute_input": "2024-07-25T04:19:30.594909Z",
          "iopub.status.idle": "2024-07-25T04:19:30.654978Z",
          "shell.execute_reply.started": "2024-07-25T04:19:30.594881Z",
          "shell.execute_reply": "2024-07-25T04:19:30.654007Z"
        },
        "trusted": true,
        "id": "FtQb2fY9UArL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### we have some missing text and location\n",
        "the missing locations are going to become a test file and the missing text are going to be dropped cuz we don't have any informations from them"
      ],
      "metadata": {
        "id": "KKZwgMHgUArL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainclean = train.dropna(subset=['text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:35:59.185193Z",
          "iopub.execute_input": "2024-07-25T04:35:59.185848Z",
          "iopub.status.idle": "2024-07-25T04:35:59.201904Z",
          "shell.execute_reply.started": "2024-07-25T04:35:59.185815Z",
          "shell.execute_reply": "2024-07-25T04:35:59.200948Z"
        },
        "trusted": true,
        "id": "9rK37bGRUArN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testsup = trainclean[trainclean[\"location\"].isna()]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:36:01.548748Z",
          "iopub.execute_input": "2024-07-25T04:36:01.549624Z",
          "iopub.status.idle": "2024-07-25T04:36:01.556974Z",
          "shell.execute_reply.started": "2024-07-25T04:36:01.549592Z",
          "shell.execute_reply": "2024-07-25T04:36:01.555971Z"
        },
        "trusted": true,
        "id": "XMoKe4sAUArN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testsup"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:36:01.835797Z",
          "iopub.execute_input": "2024-07-25T04:36:01.836071Z",
          "iopub.status.idle": "2024-07-25T04:36:01.847265Z",
          "shell.execute_reply.started": "2024-07-25T04:36:01.836048Z",
          "shell.execute_reply": "2024-07-25T04:36:01.846328Z"
        },
        "trusted": true,
        "id": "u2lix6umUArO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(trainclean.shape)\n",
        "print(test.shape)\n",
        "print(testsup.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:36:02.707935Z",
          "iopub.execute_input": "2024-07-25T04:36:02.70828Z",
          "iopub.status.idle": "2024-07-25T04:36:02.713645Z",
          "shell.execute_reply.started": "2024-07-25T04:36:02.708254Z",
          "shell.execute_reply": "2024-07-25T04:36:02.712719Z"
        },
        "trusted": true,
        "id": "_R_uyivoUArO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainclean.dropna(inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:36:04.161205Z",
          "iopub.execute_input": "2024-07-25T04:36:04.162065Z",
          "iopub.status.idle": "2024-07-25T04:36:04.175029Z",
          "shell.execute_reply.started": "2024-07-25T04:36:04.162031Z",
          "shell.execute_reply": "2024-07-25T04:36:04.174014Z"
        },
        "trusted": true,
        "id": "M9bicdNWUArP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(trainclean.shape)\n",
        "print(test.shape)\n",
        "print(testsup.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:36:04.802758Z",
          "iopub.execute_input": "2024-07-25T04:36:04.803266Z",
          "iopub.status.idle": "2024-07-25T04:36:04.807892Z",
          "shell.execute_reply.started": "2024-07-25T04:36:04.80324Z",
          "shell.execute_reply": "2024-07-25T04:36:04.807086Z"
        },
        "trusted": true,
        "id": "kiGKTMcmUArQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainclean.reset_index(drop=True,inplace = True)\n",
        "testsup.reset_index(drop=True,inplace = True)\n",
        "train.reset_index(drop=True,inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:19:38.980909Z",
          "iopub.execute_input": "2024-07-25T04:19:38.981622Z",
          "iopub.status.idle": "2024-07-25T04:19:38.986409Z",
          "shell.execute_reply.started": "2024-07-25T04:19:38.981592Z",
          "shell.execute_reply": "2024-07-25T04:19:38.985447Z"
        },
        "trusted": true,
        "id": "1w24i2ZAUArQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "w2ck2KHsUArR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is the first idea that comes to mind!:"
      ],
      "metadata": {
        "id": "SpZdp56CUArR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "locations = trainclean.location.unique()\n",
        "locations.tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T03:57:10.393807Z",
          "iopub.execute_input": "2024-07-25T03:57:10.394403Z",
          "iopub.status.idle": "2024-07-25T03:57:10.424187Z",
          "shell.execute_reply.started": "2024-07-25T03:57:10.394367Z",
          "shell.execute_reply": "2024-07-25T03:57:10.423381Z"
        },
        "trusted": true,
        "id": "YQUIUtaQUArR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T03:57:10.425272Z",
          "iopub.execute_input": "2024-07-25T03:57:10.425526Z",
          "iopub.status.idle": "2024-07-25T03:57:19.490438Z",
          "shell.execute_reply.started": "2024-07-25T03:57:10.425498Z",
          "shell.execute_reply": "2024-07-25T03:57:19.489657Z"
        },
        "trusted": true,
        "id": "FWM7o3XHUArR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_locations(text, location_list):\n",
        "    doc = nlp(text)\n",
        "    locations_found = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == 'GPE' and ent.text in location_list:\n",
        "            locations_found.append(ent.text)\n",
        "    return locations_found"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T14:50:19.304769Z",
          "iopub.execute_input": "2024-07-24T14:50:19.305334Z",
          "iopub.status.idle": "2024-07-24T14:50:19.310583Z",
          "shell.execute_reply.started": "2024-07-24T14:50:19.305309Z",
          "shell.execute_reply": "2024-07-24T14:50:19.309588Z"
        },
        "trusted": true,
        "id": "-ZllEO-mUArS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testsup['location'] = testsup['text'].apply(lambda x: extract_locations(x, locations))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T14:50:19.311589Z",
          "iopub.execute_input": "2024-07-24T14:50:19.311871Z",
          "iopub.status.idle": "2024-07-24T14:50:57.935505Z",
          "shell.execute_reply.started": "2024-07-24T14:50:19.311847Z",
          "shell.execute_reply": "2024-07-24T14:50:57.934539Z"
        },
        "trusted": true,
        "id": "mh45ladLUArS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testsup['location'] = testsup['location'].astype(str)\n",
        "testsup['location'] = testsup['location'].str.strip('[]').str.strip()\n",
        "testsup['location'] = testsup['location'].str.replace(\"'\", \"\")\n",
        "testsup['location'] = testsup['location'].str.replace(\",\", \"\")\n",
        "testsup['location'] = testsup['location'].replace(r'^\\s*$', '', regex=True)\n",
        "testsup"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T14:50:57.937237Z",
          "iopub.execute_input": "2024-07-24T14:50:57.937517Z",
          "iopub.status.idle": "2024-07-24T14:50:57.970592Z",
          "shell.execute_reply.started": "2024-07-24T14:50:57.937492Z",
          "shell.execute_reply": "2024-07-24T14:50:57.969513Z"
        },
        "trusted": true,
        "id": "FpHZTy-BUArS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "VmLysfciUArS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's add to the train data"
      ],
      "metadata": {
        "id": "tDB0_cP_UArS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testsup.drop('location',axis=1,inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:20:03.184131Z",
          "iopub.execute_input": "2024-07-25T04:20:03.184732Z",
          "iopub.status.idle": "2024-07-25T04:20:03.191051Z",
          "shell.execute_reply.started": "2024-07-25T04:20:03.184699Z",
          "shell.execute_reply": "2024-07-25T04:20:03.190174Z"
        },
        "trusted": true,
        "id": "Ea_iZpPWUArS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract locations\n",
        "def extract_locations(text):\n",
        "    doc = nlp(text)\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
        "    return ' '.join(locations)\n",
        "\n",
        "# Apply the function to the training data\n",
        "trainclean['extracted_locations'] = trainclean['text'].apply(extract_locations)\n",
        "\n",
        "# Initialize TfidfVectorizer and fit it on the training data\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train = vectorizer.fit_transform(trainclean['extracted_locations'])\n",
        "\n",
        "# Assume 'location' is your target column\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, trainclean['location'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and print classification report\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Apply the function to the test data\n",
        "testsup['extracted_locations'] = testsup['text'].apply(extract_locations)\n",
        "\n",
        "# Transform the test data using the same vectorizer\n",
        "X_test_transformed = vectorizer.transform(testsup['extracted_locations'])\n",
        "\n",
        "# Predict on the transformed test data\n",
        "y_pred = model.predict(X_test_transformed)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "import pandas as pd\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'tweet_id': testsup['tweet_id'],  # Replace with your actual ID column\n",
        "    'location': y_pred\n",
        "})\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-23T16:16:30.443108Z",
          "iopub.execute_input": "2024-07-23T16:16:30.443503Z",
          "iopub.status.idle": "2024-07-23T16:16:30.449875Z",
          "shell.execute_reply.started": "2024-07-23T16:16:30.443471Z",
          "shell.execute_reply": "2024-07-23T16:16:30.448679Z"
        },
        "trusted": true,
        "id": "YTqqjXHmUArT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# submission2 = pd.read_csv(\"/kaggle/working/submission.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-23T16:18:25.506562Z",
          "iopub.execute_input": "2024-07-23T16:18:25.50694Z",
          "iopub.status.idle": "2024-07-23T16:18:25.512472Z",
          "shell.execute_reply.started": "2024-07-23T16:18:25.506913Z",
          "shell.execute_reply": "2024-07-23T16:18:25.510975Z"
        },
        "trusted": true,
        "id": "5q27Vb96UArU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = BertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
        "\n",
        "# Create NER pipeline\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def extract_locations(text):\n",
        "    ner_results = nlp(text)\n",
        "    locations = [result['word'] for result in ner_results if result['entity'] == 'B-LOC' or result['entity'] == 'I-LOC']\n",
        "    return ' '.join(locations)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(trainclean['text'], trainclean['location'], test_size=0.2, random_state=42)\n",
        "# Apply the function to the text column\n",
        "X_train_extracted = X_train.apply(lambda x: extract_locations(x) if isinstance(x, str) else '')\n",
        "X_test_extracted = X_test.apply(lambda x: extract_locations(x) if isinstance(x, str) else '')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train_extracted)\n",
        "X_test_vectorized = vectorizer.transform(X_test_extracted)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Train a RandomForestClassifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate accuracy and other metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T14:14:55.810107Z",
          "iopub.execute_input": "2024-07-24T14:14:55.810789Z",
          "iopub.status.idle": "2024-07-24T14:14:55.81648Z",
          "shell.execute_reply.started": "2024-07-24T14:14:55.810758Z",
          "shell.execute_reply": "2024-07-24T14:14:55.815502Z"
        },
        "trusted": true,
        "id": "CBFaI7nHUArU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'tweet_id': test['tweet_id'],  # Replace with your actual ID column\n",
        "    'location': test[\"extracted_locations\"]\n",
        "})\n",
        "submission.to_csv('submission2.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-23T19:42:40.946784Z",
          "iopub.execute_input": "2024-07-23T19:42:40.94717Z",
          "iopub.status.idle": "2024-07-23T19:42:40.962454Z",
          "shell.execute_reply.started": "2024-07-23T19:42:40.94714Z",
          "shell.execute_reply": "2024-07-23T19:42:40.961494Z"
        },
        "trusted": true,
        "id": "puRWo1EKUArU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### let's handle the Hashtags and @s"
      ],
      "metadata": {
        "id": "--bW3GJlUArV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model_ner = BertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english').to('cuda')\n",
        "\n",
        "# Create NER pipeline\n",
        "nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer, device=0)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Replace @ with 'at' and extract hashtags.\"\"\"\n",
        "    text = text.replace('@', ' at ')\n",
        "    hashtags = re.findall(r'#(\\w+)', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    hashtag_locations = ' '.join(hashtags)\n",
        "    return text, hashtag_locations\n",
        "\n",
        "def extract_locations(text):\n",
        "    \"\"\"Extract locations using BERT NER.\"\"\"\n",
        "    ner_results = nlp(text)\n",
        "    locations = [result['word'] for result in ner_results if result['entity'] == 'B-LOC' or result['entity'] == 'I-LOC']\n",
        "    return ' '.join(locations)\n",
        "\n",
        "# Preprocess text in both DataFrames\n",
        "trainclean['text'], trainclean['hashtag_locations'] = zip(*trainclean['text'].apply(preprocess_text))\n",
        "testsup['text'], testsup['hashtag_locations'] = zip(*testsup['text'].apply(preprocess_text))\n",
        "\n",
        "# Extract locations from the text\n",
        "trainclean['extracted_locations'] = trainclean['text'].apply(lambda x: extract_locations(x) if isinstance(x, str) else '')\n",
        "testsup['extracted_locations'] = testsup['text'].apply(lambda x: extract_locations(x) if isinstance(x, str) else '')\n",
        "\n",
        "# Combine extracted locations and hashtag locations\n",
        "trainclean['final_extracted_locations'] = trainclean[['extracted_locations', 'hashtag_locations']].apply(lambda x: ' '.join(filter(None, x)), axis=1)\n",
        "testsup['final_extracted_locations'] = testsup[['extracted_locations', 'hashtag_locations']].apply(lambda x: ' '.join(filter(None, x)), axis=1)\n",
        "\n",
        "# Vectorize the final extracted locations\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(trainclean['final_extracted_locations'])\n",
        "X_test_vectorized = vectorizer.transform(testsup['final_extracted_locations'])\n",
        "\n",
        "# Train-Test Split for training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_vectorized, trainclean['location'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the RandomForest model\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val, y_val_pred)}')\n",
        "\n",
        "# Predict on test data\n",
        "testsup['predicted_location'] = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Display results\n",
        "testsup\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:20:27.076877Z",
          "iopub.execute_input": "2024-07-25T04:20:27.077583Z"
        },
        "trusted": true,
        "id": "tTsORHm6UArV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nebraska doesn't seem like the correct prediction"
      ],
      "metadata": {
        "id": "BGrg20mtUArV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BIfuWCylUArV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add = testsup[testsup['predicted_location']!='Nebraska']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:35:30.083877Z",
          "iopub.execute_input": "2024-07-25T04:35:30.084767Z",
          "iopub.status.idle": "2024-07-25T04:35:30.094355Z",
          "shell.execute_reply.started": "2024-07-25T04:35:30.084734Z",
          "shell.execute_reply": "2024-07-25T04:35:30.093379Z"
        },
        "trusted": true,
        "id": "Jx5QCQbtUArV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:35:32.309774Z",
          "iopub.execute_input": "2024-07-25T04:35:32.310157Z",
          "iopub.status.idle": "2024-07-25T04:35:32.324529Z",
          "shell.execute_reply.started": "2024-07-25T04:35:32.310124Z",
          "shell.execute_reply": "2024-07-25T04:35:32.323519Z"
        },
        "trusted": true,
        "id": "731qBjLiUArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supp = pd.DataFrame({\n",
        "    'tweet_id': add['tweet_id'],  # Replace with your actual ID column\n",
        "    'text' : add[\"text\"],\n",
        "    'location': add[add[\"predicted_location\"]!='Nebraska'].predicted_location\n",
        "})\n",
        "supp.dropna(inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:35:48.989782Z",
          "iopub.execute_input": "2024-07-25T04:35:48.990167Z",
          "iopub.status.idle": "2024-07-25T04:35:49.001829Z",
          "shell.execute_reply.started": "2024-07-25T04:35:48.990136Z",
          "shell.execute_reply": "2024-07-25T04:35:49.000925Z"
        },
        "trusted": true,
        "id": "vDNjkTeHUArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(supp.shape)\n",
        "print(trainclean.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:35:51.193945Z",
          "iopub.execute_input": "2024-07-25T04:35:51.194332Z",
          "iopub.status.idle": "2024-07-25T04:35:51.199278Z",
          "shell.execute_reply.started": "2024-07-25T04:35:51.194304Z",
          "shell.execute_reply": "2024-07-25T04:35:51.198369Z"
        },
        "trusted": true,
        "id": "-mD9OdwBUArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_test_vectorized,X_train_vectorized,X_train, X_val, y_train, y_val,add,testsup"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:38:42.783268Z",
          "iopub.execute_input": "2024-07-25T04:38:42.783631Z",
          "iopub.status.idle": "2024-07-25T04:38:42.824365Z",
          "shell.execute_reply.started": "2024-07-25T04:38:42.783602Z",
          "shell.execute_reply": "2024-07-25T04:38:42.823276Z"
        },
        "trusted": true,
        "id": "Hjqn_eEOUArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## now we redifine trainclean to get 3 cols"
      ],
      "metadata": {
        "id": "CGfrJlcVUArW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(supp.shape)\n",
        "print(trainclean.shape)\n",
        "supp.reset_index(inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:38:54.081914Z",
          "iopub.execute_input": "2024-07-25T04:38:54.082777Z",
          "iopub.status.idle": "2024-07-25T04:38:54.088931Z",
          "shell.execute_reply.started": "2024-07-25T04:38:54.082741Z",
          "shell.execute_reply": "2024-07-25T04:38:54.088006Z"
        },
        "trusted": true,
        "id": "VQT53UWJUArW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainclean = pd.concat([trainclean,supp])\n",
        "trainclean"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:38:57.520122Z",
          "iopub.execute_input": "2024-07-25T04:38:57.520487Z",
          "iopub.status.idle": "2024-07-25T04:38:57.546032Z",
          "shell.execute_reply.started": "2024-07-25T04:38:57.520456Z",
          "shell.execute_reply": "2024-07-25T04:38:57.545004Z"
        },
        "trusted": true,
        "id": "rTvAYSahUArX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainclean.drop(\"index\",axis = 1,inplace = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:39:00.300917Z",
          "iopub.execute_input": "2024-07-25T04:39:00.301819Z",
          "iopub.status.idle": "2024-07-25T04:39:00.308466Z",
          "shell.execute_reply.started": "2024-07-25T04:39:00.301786Z",
          "shell.execute_reply": "2024-07-25T04:39:00.307477Z"
        },
        "trusted": true,
        "id": "m1JiQVDcUArX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainclean.reset_index(inplace = True)\n",
        "trainclean"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:39:04.466804Z",
          "iopub.execute_input": "2024-07-25T04:39:04.467656Z",
          "iopub.status.idle": "2024-07-25T04:39:04.480234Z",
          "shell.execute_reply.started": "2024-07-25T04:39:04.46762Z",
          "shell.execute_reply": "2024-07-25T04:39:04.479216Z"
        },
        "trusted": true,
        "id": "G-qH7cKxUArX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#del supp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:39:47.804947Z",
          "iopub.execute_input": "2024-07-25T04:39:47.80534Z",
          "iopub.status.idle": "2024-07-25T04:39:47.809398Z",
          "shell.execute_reply.started": "2024-07-25T04:39:47.805308Z",
          "shell.execute_reply": "2024-07-25T04:39:47.808498Z"
        },
        "trusted": true,
        "id": "MZx9mx8CUAre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now we train the model with the new DF"
      ],
      "metadata": {
        "id": "oe1lnf7oUArf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model_ner = BertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english').to('cuda')\n",
        "\n",
        "# Create NER pipeline\n",
        "nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer, device=0)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Replace @ with 'at' and extract hashtags.\"\"\"\n",
        "    text = text.replace('@', ' at ')\n",
        "    hashtags = re.findall(r'#(\\w+)', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    hashtag_locations = ' '.join(hashtags)\n",
        "    return text, hashtag_locations\n",
        "\n",
        "def extract_locations(text):\n",
        "    \"\"\"Extract locations using BERT NER and custom rules.\"\"\"\n",
        "    # Custom rule-based extraction for hashtags and common indicators\n",
        "    text, hashtag_locations = preprocess_text(text)\n",
        "    common_indicators = ['Municipality', 'City', 'Town', 'Village']\n",
        "    additional_locations = ' '.join([word for word in text.split() if any(ind in word for ind in common_indicators)])\n",
        "\n",
        "    ner_results = nlp(text)\n",
        "    locations = [result['word'] for result in ner_results if result['entity'] == 'B-LOC' or result['entity'] == 'I-LOC']\n",
        "\n",
        "    # Combine NER and custom rule-based locations\n",
        "    all_locations = ' '.join(locations + additional_locations.split() + hashtag_locations.split())\n",
        "    return all_locations\n",
        "\n",
        "# Preprocess text in both DataFrames\n",
        "trainclean['text'], trainclean['hashtag_locations'] = zip(*trainclean['text'].apply(preprocess_text))\n",
        "test['text'], test['hashtag_locations'] = zip(*test['text'].apply(preprocess_text))\n",
        "\n",
        "# Extract locations from the text\n",
        "trainclean['extracted_locations'] = trainclean['text'].apply(lambda x: extract_locations(x) if isinstance(x, str) else '')\n",
        "test['extracted_locations'] = test['text'].apply(lambda x: extract_locations(x) if isinstance(x, str) else '')\n",
        "\n",
        "# Combine extracted locations and hashtag locations\n",
        "trainclean['final_extracted_locations'] = trainclean[['extracted_locations', 'hashtag_locations']].apply(lambda x: ' '.join(filter(None, x)), axis=1)\n",
        "test['final_extracted_locations'] = test[['extracted_locations', 'hashtag_locations']].apply(lambda x: ' '.join(filter(None, x)), axis=1)\n",
        "\n",
        "# Vectorize the final extracted locations\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(trainclean['final_extracted_locations'])\n",
        "X_test_vectorized = vectorizer.transform(test['final_extracted_locations'])\n",
        "\n",
        "# Train-Test Split for training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_vectorized, trainclean['location'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the RandomForest model\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val, y_val_pred)}')\n",
        "\n",
        "# Predict on test data\n",
        "test['predicted_location'] = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Display results\n",
        "print(test[['text', 'final_extracted_locations', 'predicted_location']])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:39:49.130155Z",
          "iopub.execute_input": "2024-07-25T04:39:49.130774Z",
          "iopub.status.idle": "2024-07-25T04:46:30.522719Z",
          "shell.execute_reply.started": "2024-07-25T04:39:49.130741Z",
          "shell.execute_reply": "2024-07-25T04:46:30.521653Z"
        },
        "trusted": true,
        "id": "mGvQlPoMUArf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "X_-K9ju0UArf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "J_kHqz_DUArf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with some Submissions"
      ],
      "metadata": {
        "id": "e5B5frU3UArf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'tweet_id': test['tweet_id'],  # Replace with your actual ID column\n",
        "    'location': test[\"predicted_location\"]\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:57:05.272341Z",
          "iopub.execute_input": "2024-07-25T04:57:05.272741Z",
          "iopub.status.idle": "2024-07-25T04:57:05.286137Z",
          "shell.execute_reply.started": "2024-07-25T04:57:05.272709Z",
          "shell.execute_reply": "2024-07-25T04:57:05.285228Z"
        },
        "trusted": true,
        "id": "ATbsTNdXUArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission3 = pd.read_csv(\"/kaggle/input/submissions/submission3.csv\")\n",
        "submission2 = pd.read_csv(\"/kaggle/input/submissions/submission2.csv\")\n",
        "submission1 = pd.read_csv(\"/kaggle/input/submissions/submission1.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:42:20.080106Z",
          "iopub.execute_input": "2024-07-24T10:42:20.08095Z",
          "iopub.status.idle": "2024-07-24T10:42:20.12823Z",
          "shell.execute_reply.started": "2024-07-24T10:42:20.08092Z",
          "shell.execute_reply": "2024-07-24T10:42:20.127538Z"
        },
        "trusted": true,
        "id": "ae_P-MM_UArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_sub3 = (submission3[submission3[\"location\"].isna()].tweet_id).tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:42:20.4096Z",
          "iopub.execute_input": "2024-07-24T10:42:20.409952Z",
          "iopub.status.idle": "2024-07-24T10:42:20.416017Z",
          "shell.execute_reply.started": "2024-07-24T10:42:20.409922Z",
          "shell.execute_reply": "2024-07-24T10:42:20.414991Z"
        },
        "trusted": true,
        "id": "ITuPWwrVUArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_sub2 = (submission2[submission2[\"location\"].isna()].tweet_id).tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:42:21.044513Z",
          "iopub.execute_input": "2024-07-24T10:42:21.044869Z",
          "iopub.status.idle": "2024-07-24T10:42:21.051013Z",
          "shell.execute_reply.started": "2024-07-24T10:42:21.044841Z",
          "shell.execute_reply": "2024-07-24T10:42:21.049935Z"
        },
        "trusted": true,
        "id": "wnaZBIu6UArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set3 = set(missing_sub3)\n",
        "set2 = set(missing_sub2)\n",
        "missing_all = set2 | set3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:42:21.834459Z",
          "iopub.execute_input": "2024-07-24T10:42:21.835279Z",
          "iopub.status.idle": "2024-07-24T10:42:21.839578Z",
          "shell.execute_reply.started": "2024-07-24T10:42:21.835244Z",
          "shell.execute_reply": "2024-07-24T10:42:21.838663Z"
        },
        "trusted": true,
        "id": "s5Ub97W7UArh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testmissing = test[test['tweet_id'].isin(missing_all)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:42:22.037221Z",
          "iopub.execute_input": "2024-07-24T10:42:22.037896Z",
          "iopub.status.idle": "2024-07-24T10:42:22.043157Z",
          "shell.execute_reply.started": "2024-07-24T10:42:22.03787Z",
          "shell.execute_reply": "2024-07-24T10:42:22.042044Z"
        },
        "trusted": true,
        "id": "Q032-sRSUArh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testmissing.reset_index(drop=True,inplace = True)\n",
        "testmissing"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:42:22.528207Z",
          "iopub.execute_input": "2024-07-24T10:42:22.529112Z",
          "iopub.status.idle": "2024-07-24T10:42:22.539499Z",
          "shell.execute_reply.started": "2024-07-24T10:42:22.529084Z",
          "shell.execute_reply": "2024-07-24T10:42:22.5384Z"
        },
        "trusted": true,
        "id": "aXjUAwtLUArh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testmissing['text'] = testmissing['text'].str.replace('#', ' ')\n",
        "testmissing['text'] = testmissing['text'].str.replace('@', 'at ')\n",
        "testmissing"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T05:00:51.294492Z",
          "iopub.execute_input": "2024-07-25T05:00:51.295725Z",
          "iopub.status.idle": "2024-07-25T05:00:51.335732Z",
          "shell.execute_reply.started": "2024-07-25T05:00:51.295685Z",
          "shell.execute_reply": "2024-07-25T05:00:51.333506Z"
        },
        "trusted": true,
        "id": "gBZzALbQUArh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForTokenClassification, pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model_ner = BertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english').to('cuda')\n",
        "\n",
        "# Create NER pipeline\n",
        "nlp = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer, device=0)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Replace @ with 'at', extract hashtags, and clean text.\"\"\"\n",
        "    text = text.replace('@', ' at ')\n",
        "    hashtags = re.findall(r'#(\\w+)', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    hashtag_locations = ' '.join(hashtags)\n",
        "    return text, hashtag_locations\n",
        "\n",
        "def extract_locations_and_more(text):\n",
        "    \"\"\"Extract locations, roads, streets, localities, districts, and countries.\"\"\"\n",
        "    # Custom rule-based extraction for roads, streets, localities, etc.\n",
        "    road_patterns = re.findall(r'\\b(\\d+)\\b', text)  # Extract road numbers\n",
        "    street_patterns = re.findall(r'\\b(?:street|st|road|rd|avenue|ave|blvd)\\b', text, re.IGNORECASE)\n",
        "    locality_patterns = re.findall(r'\\b(?:locality|district)\\b', text, re.IGNORECASE)\n",
        "    country_patterns = re.findall(r'\\b(?:country|nation)\\b', text, re.IGNORECASE)\n",
        "\n",
        "    # Clean text\n",
        "    text, hashtag_locations = preprocess_text(text)\n",
        "\n",
        "    # Extract using BERT NER\n",
        "    ner_results = nlp(text)\n",
        "    locations = [result['word'] for result in ner_results if result['entity'] == 'B-LOC' or result['entity'] == 'I-LOC']\n",
        "\n",
        "    # Combine all extracted information\n",
        "    all_locations = ' '.join(locations + road_patterns + street_patterns + locality_patterns + country_patterns + hashtag_locations.split())\n",
        "    return all_locations\n",
        "\n",
        "# Preprocess text in both DataFrames\n",
        "trainclean['text'], trainclean['hashtag_locations'] = zip(*trainclean['text'].apply(preprocess_text))\n",
        "testmissing['text'], testmissing['hashtag_locations'] = zip(*testmissing['text'].apply(preprocess_text))\n",
        "\n",
        "# Extract locations and additional information from the text\n",
        "trainclean['extracted_info'] = trainclean['text'].apply(lambda x: extract_locations_and_more(x) if isinstance(x, str) else '')\n",
        "testmissing['extracted_info'] = testmissing['text'].apply(lambda x: extract_locations_and_more(x) if isinstance(x, str) else '')\n",
        "\n",
        "# Vectorize the extracted information\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(trainclean['extracted_info'])\n",
        "X_test_vectorized = vectorizer.transform(testmissing['extracted_info'])\n",
        "\n",
        "# Train-Test Split for training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_vectorized, trainclean['location'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the RandomForest model\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "print(f'Validation Accuracy: {accuracy_score(y_val, y_val_pred)}')\n",
        "\n",
        "# Predict on test data\n",
        "testmissing['predicted_location'] = rf_model.predict(X_test_vectorized)\n",
        "\n",
        "# Display results\n",
        "print(testmissing[['text', 'extracted_info', 'predicted_location']])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:26:01.668313Z",
          "iopub.execute_input": "2024-07-24T10:26:01.668706Z",
          "iopub.status.idle": "2024-07-24T10:30:50.350536Z",
          "shell.execute_reply.started": "2024-07-24T10:26:01.668678Z",
          "shell.execute_reply": "2024-07-24T10:30:50.349667Z"
        },
        "trusted": true,
        "id": "BDNW4yjUUAri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "uB--DqrGUAri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'tweet_id': testmissing['tweet_id'],  # Replace with your actual ID column\n",
        "    'location': testmissing[\"predicted_location\"]\n",
        "})\n",
        "submission.to_csv('submission22.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:31:22.49643Z",
          "iopub.execute_input": "2024-07-24T10:31:22.497554Z",
          "iopub.status.idle": "2024-07-24T10:31:22.504444Z",
          "shell.execute_reply.started": "2024-07-24T10:31:22.497324Z",
          "shell.execute_reply": "2024-07-24T10:31:22.503606Z"
        },
        "trusted": true,
        "id": "70aNwbYbUAri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submissing1 = pd.read_csv(\"/kaggle/working/submissionmissing.csv\")\n",
        "submissing2 = pd.read_csv(\"/kaggle/working/submission22.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T10:42:35.737485Z",
          "iopub.execute_input": "2024-07-24T10:42:35.737876Z",
          "iopub.status.idle": "2024-07-24T10:42:35.746999Z",
          "shell.execute_reply.started": "2024-07-24T10:42:35.737845Z",
          "shell.execute_reply": "2024-07-24T10:42:35.745928Z"
        },
        "trusted": true,
        "id": "u18tOWdMUArj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "yIpP_ImeUArj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "h6BrajN1UArj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "JWdQ3pHlUArj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'tweet_id': result1['tweet_id'],  # Replace with your actual ID column\n",
        "    'location': result1[\"result\"]\n",
        "})\n",
        "submission.to_csv('subxx.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-24T11:52:53.921231Z",
          "iopub.execute_input": "2024-07-24T11:52:53.9222Z",
          "iopub.status.idle": "2024-07-24T11:52:53.936576Z",
          "shell.execute_reply.started": "2024-07-24T11:52:53.922159Z",
          "shell.execute_reply": "2024-07-24T11:52:53.935566Z"
        },
        "trusted": true,
        "id": "C6WxySsIUArj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUcVmsSjUArk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}